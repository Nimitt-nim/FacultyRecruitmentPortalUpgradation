{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Journal Name and Authors From DOI number\n",
    "\n",
    "import requests\n",
    "def get_journal_and_authors_from_doi(doi):\n",
    "    url = f\"https://api.crossref.org/works/{doi}\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            journal_name = data['message']['container-title'][0]  \n",
    "            authors = data['message']['author']\n",
    "            author_names = [f\"{author['given']} {author['family']}\" for author in authors]\n",
    "            return journal_name, author_names\n",
    "        else:\n",
    "            print(f\"Error 2. Status code: {response.status_code}\")\n",
    "    except:\n",
    "        print(\"Error 1.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the Link of Journal Data page from main ScimagoJR page\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_page(journal_name):\n",
    "  search_name=journal_name.strip().replace(\" \",\"+\")\n",
    "  search_name=search_name.replace(':','%3A')\n",
    "  search_url=f\"https://www.scimagojr.com/journalsearch.php?q={search_name}\"\n",
    "  r = requests.get(search_url)\n",
    "  if r.status_code==200:\n",
    "    soup = BeautifulSoup(r.content, 'html5lib')\n",
    "    try:\n",
    "      search_result=soup.find('div',class_=\"search_results\").find(\"a\")['href']\n",
    "    except TypeError:\n",
    "      search_result=None\n",
    "      print(journal_name,\"No results were found\")\n",
    "  else:\n",
    "    search_result=None\n",
    "    print(journal_name,'Page not found 0')\n",
    "  return search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting Quartile from Journal Data\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def quartile_list(search_result):\n",
    "    if search_result is not None:\n",
    "        result_url = f'https://www.scimagojr.com/{search_result}'\n",
    "        try:\n",
    "            response = requests.get(result_url)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            quartile_th = soup.find('th', text='Quartile')\n",
    "            if quartile_th:\n",
    "                quartile_table = quartile_th.find_parent('table')\n",
    "                if quartile_table:\n",
    "                    df = pd.read_html(str(quartile_table), match='Quartile')[0]\n",
    "                    latest_rating_indices = df.groupby('Category').Year.agg('idxmax')\n",
    "                    latest_ratings = df.loc[latest_rating_indices]\n",
    "                    unique_ratings = latest_ratings['Quartile'].unique()[0]\n",
    "                else:\n",
    "                    unique_ratings = None\n",
    "                    print(result_url, \"No quartile category found\")\n",
    "            else:\n",
    "                unique_ratings = None\n",
    "                print(result_url, \"No quartile found on page\")\n",
    "        except ValueError:\n",
    "            unique_ratings = None\n",
    "        except requests.HTTPError:\n",
    "            unique_ratings = None\n",
    "            print(\"Page Not Found\")\n",
    "    else:\n",
    "        unique_ratings = None\n",
    "    return unique_ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting DOI data from Google Spreadsheet\n",
    "\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import numpy as np\n",
    "def get_doi_list_from_spreadsheet(sheetName):\n",
    "    scope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/drive\"]\n",
    "    credentials = ServiceAccountCredentials.from_json_keyfile_name(\"facultyrecruitmentupgradation-5e06eb0edef3.json\", scope)\n",
    "\n",
    "    gc = gspread.authorize(credentials)\n",
    "    worksheet = gc.open(sheetName).sheet1\n",
    "\n",
    "    data = worksheet.get_all_records()\n",
    "    doi_dataframe = pd.DataFrame(data)\n",
    "    doi_data = doi_dataframe['DOI NUMBER']\n",
    "    doi_list = np.array(doi_data)\n",
    "\n",
    "    for i in range(len(doi_list)):\n",
    "        if str(doi_list[i][0:15]) == \"https://doi.org\":\n",
    "            doi_list[i] = doi_list[i][16:]\n",
    "    return doi_list\n",
    "sheetName = \"ApplicationsData\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post Processing Incorrect Entries\n",
    "\n",
    "def post_process_invalid_inputs(Journal_name,doi):\n",
    "    Journal_name = Journal_name.split(':')[0]\n",
    "    return Journal_name,doi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing into Google Spreadsheet again\n",
    "  \n",
    "def enter_into_sheet(df,sheetName):\n",
    "        scope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/drive\"]\n",
    "        credentials = ServiceAccountCredentials.from_json_keyfile_name('facultyrecruitmentupgradation-5e06eb0edef3.json', scope)\n",
    "        gc = gspread.authorize(credentials)\n",
    "        spreadsheet = gc.open(sheetName)\n",
    "\n",
    "        worksheet = spreadsheet.worksheet('Sheet1')  \n",
    "        data_sheet = []\n",
    "        n = len(df)\n",
    "        for i in range(n):\n",
    "                data_sheet.append(np.array(df.loc[i]).tolist()) \n",
    "        worksheet.insert_rows(data_sheet, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df  = pd.DataFrame({1 : [\"nim\"],2 : [\"sim\"]})\n",
    "df\n",
    "enter_into_sheet(df,sheetName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "\n",
    "sheetName = \"ApplicationsData\"\n",
    "doi_list = get_doi_list_from_spreadsheet(sheetName)\n",
    "data=[]\n",
    "\n",
    "for doi in doi_list:\n",
    "  try :\n",
    "    journal_name, author_names = get_journal_and_authors_from_doi(doi)\n",
    "  except :\n",
    "    continue\n",
    "  search_result=search_page(journal_name)\n",
    "  quartile=quartile_list(search_result)\n",
    "  if quartile == None:\n",
    "    journal_name = post_process_invalid_inputs(journal_name,doi)[0]\n",
    "  search_result=search_page(journal_name)\n",
    "  quartile=quartile_list(search_result)\n",
    "  data.append([doi,journal_name,quartile,len(author_names)])\n",
    "\n",
    "df=pd.DataFrame(data,columns=['DOI','Journal name','Category','No of authors'])\n",
    "\n",
    "enter_into_sheet(df,sheetName)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
